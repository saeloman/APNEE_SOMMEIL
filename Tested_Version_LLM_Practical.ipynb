{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeloman/APNEE_SOMMEIL/blob/main/Tested_Version_LLM_Practical.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2s4kN_QPQVe"
      },
      "source": [
        "# LLM Practical\n",
        "\n",
        "<img src=\"https://www.marktechpost.com/wp-content/uploads/2023/05/Blog-Banner-3.jpg\" width=\"60%\" />\n",
        "\n",
        "\n",
        "¬© .\n",
        "\n",
        "**Author: ALIAMINI **\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "Bienvenue dans votre Practical sur les LLMs ‚Äîvotre porte d'entr√©e vers le monde fascinant des Mod√®les de Langage de Grande Taille !\n",
        "\n",
        "Dans ce tutoriel, vous aurez l'occasion de vous exercer √† entra√Æner votre propre Mod√®le de Langage (LLM) ! Pr√©parez-vous √† explorer comment ces syst√®mes d'IA impressionnants cr√©ent des textes aussi r√©alistes et captivants. Partons ensemble pour ce voyage passionnant et d√©verrouillons les secrets des LLMs ! üöÄüìö\n",
        "\n",
        "**Pr√©requis :**\n",
        "\n",
        "* Bases en Python.\n",
        "* Connaissances introductives en Machine Learning.\n",
        "* Connaissances introductives en NLP.\n",
        "\n",
        "NB: Ce notebook utilise le framework JAX.\n",
        "\n",
        "**Plan :**\n",
        "\n",
        ">[Installations & Importations](#scrollTo=6EqhIg1odqg0)\n",
        "\n",
        ">[Entra√Ænement de votre LLM](#scrollTo=wmt3tp38G90A)\n",
        "\n",
        ">>[1. Objectif d'entra√Ænement Interm√©diaire](#scrollTo=jGKuXIJJyyo8)\n",
        "\n",
        ">>[2. Entra√Ænement des mod√®les Avanc√©s](#scrollTo=4CSfvGj__RGA)\n",
        "\n",
        ">>[3. Inspecter le LLM entra√Æn√© D√©butant](#scrollTo=pGv9c2AFmF4V)\n",
        "\n",
        ">[Conclusion](#scrollTo=15296-QL3-y3)\n",
        "\n",
        "**Avant de commencer :**\n",
        "\n",
        "Pour ce TP, vous aurez besoin d'utiliser un GPU pour acc√©l√©rer l'entra√Ænement. Pour ce faire, allez dans le menu \"Ex√©cution\" de Colab, s√©lectionnez \"Modifier le type d'ex√©cution\", puis dans le menu popup, choisissez \"T4 GPU\" dans la bo√Æte \"Acc√©l√©rateur mat√©riel\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DKJ2Hc1zzadi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EqhIg1odqg0"
      },
      "source": [
        "## Installations & Importations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boGA9rYdt9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc9fb245-8428-40cc-9036-4e767766dcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Collecting umap-learn\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.8.0)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.5.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.13 umap-learn-0.5.7\n",
            "Collecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.8.0)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from livelossplot) (3.6.1)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (1.26.4)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (24.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2.2.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->livelossplot) (2024.9.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (3.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->bokeh->livelossplot) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->livelossplot) (1.16.0)\n",
            "Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.5\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.26.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.9.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.8.30)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUn GPU est connect√©.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package word2vec_sample to /root/nltk_data...\n",
            "[nltk_data]   Unzipping models/word2vec_sample.zip.\n"
          ]
        }
      ],
      "source": [
        "# Installer les biblioth√®ques n√©cessaires pour le deep learning, le NLP et la visualisation\n",
        "!pip install transformers datasets  # Biblioth√®ques Transformers et datasets pour les t√¢ches de NLP\n",
        "!pip install seaborn umap-learn     # Seaborn pour la visualisation, UMAP pour la r√©duction dimensionnelle\n",
        "!pip install livelossplot           # LiveLossPlot pour suivre les progr√®s de l'entra√Ænement du mod√®le\n",
        "!pip install -q transformers[torch] # Transformers avec le backend PyTorch\n",
        "!pip install -q peft                # Biblioth√®que de fine-tuning efficient en param√®tres\n",
        "!pip install accelerate -U          # Biblioth√®que Accelerate pour les performances\n",
        "\n",
        "# Installer des utilitaires pour le d√©bogage et le formatage de la sortie console\n",
        "!pip install -q ipdb                # D√©bogueur interactif Python\n",
        "!pip install -q colorama            # Sortie de texte color√©e dans le terminal\n",
        "\n",
        "# Importer des utilitaires syst√®me et math√©matiques\n",
        "import os\n",
        "import math\n",
        "import urllib.request\n",
        "\n",
        "# V√©rifier les acc√©l√©rateurs connect√©s (GPU ou TPU) et configurer en cons√©quence\n",
        "if os.environ.get(\"COLAB_GPU\") and int(os.environ[\"COLAB_GPU\"]) > 0:\n",
        "    print(\"Un GPU est connect√©.\")\n",
        "elif \"COLAB_TPU_ADDR\" in os.environ and os.environ[\"COLAB_TPU_ADDR\"]:\n",
        "    print(\"Un TPU est connect√©.\")\n",
        "    import jax.tools.colab_tpu\n",
        "    jax.tools.colab_tpu.setup_tpu()\n",
        "else:\n",
        "    print(\"Seul le processeur (CPU) est connect√©.\")\n",
        "\n",
        "# √âviter que l'allocation de m√©moire GPU soit effectu√©e par JAX\n",
        "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = \"false\"\n",
        "\n",
        "# Importer les biblioth√®ques pour le deep learning bas√© sur JAX\n",
        "import chex\n",
        "import flax\n",
        "import flax.linen as nn\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "import optax\n",
        "\n",
        "# Importer les biblioth√®ques li√©es au NLP et aux mod√®les\n",
        "import transformers\n",
        "from transformers import pipeline, AutoTokenizer, AutoModel\n",
        "import datasets\n",
        "import peft\n",
        "\n",
        "# Importer les biblioth√®ques pour le traitement d'images et la visualisation\n",
        "from PIL import Image\n",
        "from livelossplot import PlotLosses\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "# Importer des utilitaires suppl√©mentaires pour travailler avec le texte et les mod√®les\n",
        "import torch\n",
        "import torchvision\n",
        "import itertools\n",
        "import random\n",
        "import copy\n",
        "\n",
        "# T√©l√©charger une image d'exemple √† utiliser dans le notebook\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://images.unsplash.com/photo-1529778873920-4da4926a72c2?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxzZWFyY2h8MXx8Y3V0ZSUyMGNhdHxlbnwwfHwwfHw%3D&w=1000&q=80\",\n",
        "    \"cat.png\",\n",
        ")\n",
        "\n",
        "# Importer les biblioth√®ques pour le pr√©traitement NLP et le travail avec des mod√®les pr√©-entra√Æn√©s\n",
        "import gensim\n",
        "from nltk.data import find\n",
        "import nltk\n",
        "nltk.download(\"word2vec_sample\")\n",
        "\n",
        "# Importer les outils Hugging Face et les widgets IPython\n",
        "import huggingface_hub\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "import colorama\n",
        "\n",
        "# Configurer Matplotlib pour g√©n√©rer des graphiques au format SVG pour une meilleure qualit√©\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmt3tp38G90A"
      },
      "source": [
        "## Entra√Ænement de votre LLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Objectif d'entra√Ænement <font color='green'>Interm√©diaire</font>"
      ],
      "metadata": {
        "id": "jGKuXIJJyyo8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOSv1-3B_RGA"
      },
      "source": [
        "Une phrase n'est rien d'autre qu'une cha√Æne de mots. Un LLM vise √† pr√©dire le mot suivant en tenant compte du contexte actuel, c'est-√†-dire des mots qui l'ont pr√©c√©d√©.\n",
        "\n",
        "Voici l'id√©e de base :\n",
        "\n",
        "Pour calculer la probabilit√© d'une phrase compl√®te \"mot1, mot2, ..., dernier mot\" apparaissant dans un contexte donn√© $c$, la proc√©dure consiste √† d√©composer la phrase en mots individuels et √† consid√©rer la probabilit√© de chaque mot √©tant donn√© les mots qui le pr√©c√®dent. Ces probabilit√©s individuelles sont ensuite multipli√©es ensemble :\n",
        "\n",
        "$$\\text{Probabilit√© de la phrase} = \\text{Probabilit√© de mot1} \\times \\text{Probabilit√© de mot2} \\times \\ldots \\times \\text{Probabilit√© du dernier mot}$$\n",
        "\n",
        "Cette m√©thode est semblable √† la construction d'une narration pi√®ce par pi√®ce en fonction de l'histoire pr√©c√©dente.\n",
        "\n",
        "Math√©matiquement, cela s'exprime comme la vraisemblance (probabilit√©) d'une s√©quence de mots $y_1, y_2, ..., y_n$ dans un contexte donn√© $c$, ce qui est r√©alis√© en multipliant les probabilit√©s de chaque mot $y_t$ calcul√©es √©tant donn√© les pr√©d√©cesseurs ($y_{<t}$) et le contexte $c$ :\n",
        "\n",
        "$$\n",
        "P\\left(y_{1}, y_{2}, \\ldots, y_{n}, \\mid c\\right)=\\prod_{t=1}^{n} P\\left(y_{t} \\mid y_{<t}, c\\right)\n",
        "$$\n",
        "\n",
        "Ici $y_{<t}$ repr√©sente la s√©quence $y_1, y_2, ..., y_{t-1}$, tandis que $c$ repr√©sente le contexte.\n",
        "\n",
        "Cela est analogue √† r√©soudre un puzzle o√π la pi√®ce suivante est plac√©e pr√©visiblement en fonction de ce qui est d√©j√† en place.\n",
        "\n",
        "Rappelez-vous que lors de l'entra√Ænement d'un transformateur, nous ne travaillons pas avec des mots, mais avec des tokens. Pendant le processus d'entra√Ænement, les param√®tres du mod√®le sont affin√©s en calculant la perte de l'entropie crois√©e entre le token pr√©dit et le token correct, puis en effectuant une r√©tropropagation. La perte pour l'√©tape temporelle \"t\" est calcul√©e comme suit :\n",
        "\n",
        "$$ \\text{Perte}_t = - \\sum_{w \\in V} y_t\\log (\\hat{y}_t) $$\n",
        "\n",
        "Ici $y_t$ est le token r√©el √† l'√©tape temporelle $t$, et $\\hat{y}_t$ est le token pr√©dit par le mod√®le √† la m√™me √©tape temporelle. La perte pour l'ensemble de la phrase est ensuite calcul√©e comme suit :\n",
        "\n",
        "$$ \\text{Perte de la phrase} = \\frac{1}{n} \\sum^{n}_{t=1} \\text{Perte}_t $$\n",
        "\n",
        "o√π $n$ est la longueur de la s√©quence.\n",
        "\n",
        "Ce processus it√©ratif affine finalement les capacit√©s pr√©dictives du mod√®le au fil du temps.\n",
        "\n",
        "**T√¢che de code** : Impl√©mentez la fonction de perte d'entropie crois√©e ci-dessous.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXmjUYdDHseM"
      },
      "outputs": [],
      "source": [
        "def sequence_loss_fn(logits, targets):\n",
        "  '''\n",
        "  Calculer la perte d'entropie crois√©e entre l'ID de token pr√©dit et l'ID r√©el.\n",
        "\n",
        "  Args:\n",
        "    logits: Un tableau de forme [batch_size, sequence_length, vocab_size]\n",
        "    targets: Les IDs de token r√©els que nous essayons de pr√©dire, forme [batch_size, sequence_length]\n",
        "\n",
        "  Returns:\n",
        "    loss: Une valeur scalaire repr√©sentant la perte moyenne du lot\n",
        "  '''\n",
        "\n",
        "  target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "  assert logits.shape == target_labels.shape\n",
        "\n",
        "  per_token_loss = -jnp.sum(target_labels* jnp.log_softmax(logits),axis=-1)\n",
        "\n",
        "  mask = jnp.greater(targets, 0)\n",
        "\n",
        "  masked_loos = jnp.sum(per_token_loss*mask)/jnp.sum(mask)\n",
        "\n",
        "  loss = ... # FINIR MOI\n",
        "\n",
        "  return loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cq5_4WN_RGA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed3fec88-2f58-446e-8436-217b810224f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Il semble correct. Consultez la r√©ponse ci-dessous pour comparer les m√©thodes.\n"
          ]
        }
      ],
      "source": [
        "# @title Ex√©cutez-moi pour tester votre code\n",
        "VOCAB_SIZE = 25670\n",
        "targets = jnp.array([[0, 2, 0]])\n",
        "key = jax.random.PRNGKey(42)\n",
        "X = jax.random.normal(key, [1, 3, VOCAB_SIZE])\n",
        "loss = sequence_loss_fn(X, targets)\n",
        "real_loss = jnp.array(10.966118)\n",
        "assert jnp.allclose(real_loss, loss), \"La valeur retourn√©e n'est pas correcte\"\n",
        "print(\"Il semble correct. Consultez la r√©ponse ci-dessous pour comparer les m√©thodes.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cthfcbmC_RGA"
      },
      "outputs": [],
      "source": [
        "# @title R√©ponse √† la t√¢che de code (Essayez de ne pas regarder avant d'avoir bien essay√© !)\n",
        "def sequence_loss_fn(logits, targets):\n",
        "    \"\"\"Calculer la perte de s√©quence entre les logits pr√©dits et les √©tiquettes cibles.\"\"\"\n",
        "\n",
        "    # Convertir les indices cibles en vecteurs encod√©s en one-hot.\n",
        "    # Chaque √©tiquette cible est convertie en un vecteur one-hot de taille VOCAB_SIZE.\n",
        "    target_labels = jax.nn.one_hot(targets, VOCAB_SIZE)\n",
        "\n",
        "    # Assurer que la forme des logits correspond √† la forme des cibles encod√©es en one-hot.\n",
        "    # C'est important car nous devons calculer la perte sur les dimensions correspondantes.\n",
        "    assert logits.shape == target_labels.shape\n",
        "\n",
        "    # Cr√©er un masque qui ignore les jetons de padding dans le calcul de la perte.\n",
        "    # Le masque est True (1) lorsque la valeur cible est sup√©rieure √† 0 et False (0) sinon.\n",
        "    mask = jnp.greater(targets, 0)\n",
        "\n",
        "    # Calculer la perte d'entropie crois√©e pour chaque jeton.\n",
        "    # L'entropie crois√©e est calcul√©e comme le logarithme n√©gatif de la probabilit√© de la classe correcte.\n",
        "    # jax.nn.log_softmax(logits) nous donne les probabilit√©s logarithmiques pour chaque classe.\n",
        "    # Nous multiplions par les target_labels pour s√©lectionner la probabilit√© logarithmique de la classe correcte.\n",
        "    loss = -jnp.sum(target_labels * jax.nn.log_softmax(logits), axis=-1)\n",
        "\n",
        "    # Appliquer le masque √† la perte pour ignorer les positions de padding et additionner les pertes.\n",
        "    # Nous normalisons ensuite la perte totale par le nombre de jetons non-padding.\n",
        "    loss = jnp.sum(loss * mask) / jnp.sum(mask)\n",
        "\n",
        "    return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CSfvGj__RGA"
      },
      "source": [
        "### 2. Entra√Ænement des mod√®les <font color='blue'>Avanc√©</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIQ_aJGW_RGA"
      },
      "source": [
        "Dans la section suivante, nous d√©finissons tous les processus n√©cessaires pour entra√Æner le mod√®le en utilisant l'objectif d√©crit ci-dessus. Une grande partie de cela concerne maintenant le travail requis pour effectuer l'entra√Ænement avec FLAX.\n",
        "\n",
        "Ci-dessous, nous rassemblons le jeu de donn√©es sur lequel nous allons entra√Æner, qui est le jeu de donn√©es de Shakespeare de Karpathy. Il n'est pas si important de comprendre ce code, donc soit ex√©cutez simplement la cellule pour charger les donn√©es, soit consultez le code si vous souhaitez le comprendre.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guMHAaSo_RGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bf0166b-fffb-4256-cef5-8f8cf33c91cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-23 12:40:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‚Äòinput.txt‚Äô\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-11-23 12:40:27 (28.6 MB/s) - ‚Äòinput.txt‚Äô saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Cr√©er le jeu de donn√©es Shakespeare et l'it√©rateur (optionnel, mais ex√©cutez la cellule)\n",
        "\n",
        "# Astuce pour √©viter les erreurs lors du t√©l√©chargement de tinyshakespeare.\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O input.txt\n",
        "\n",
        "class WordBasedAsciiDatasetForLLM:\n",
        "    \"\"\"Jeu de donn√©es en m√©moire d'un fichier ASCII unique pour un mod√®le de type langage.\"\"\"\n",
        "\n",
        "    def __init__(self, path: str, batch_size: int, sequence_length: int):\n",
        "        \"\"\"Charger un fichier ASCII unique en m√©moire.\"\"\"\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        with open(path, \"r\") as f:\n",
        "            corpus = f.read()\n",
        "\n",
        "        # Tokeniser en s√©parant le texte en mots\n",
        "        words = corpus.split()\n",
        "        self.vocab_size = len(set(words))  # Nombre de mots uniques\n",
        "\n",
        "        # Cr√©er un mapping de mots vers des IDs uniques\n",
        "        self.word_to_id = {word: i for i, word in enumerate(set(words))}\n",
        "\n",
        "        # Stocker le mapping inverse des IDs vers les mots\n",
        "        self.id_to_word = {i: word for word, i in self.word_to_id.items()}\n",
        "\n",
        "        # Convertir les mots du corpus en leurs IDs correspondants\n",
        "        corpus = np.array([self.word_to_id[word] for word in words]).astype(np.int32)\n",
        "\n",
        "        crop_len = sequence_length + 1\n",
        "        num_batches, ragged = divmod(corpus.size, batch_size * crop_len)\n",
        "        if ragged:\n",
        "            corpus = corpus[:-ragged]\n",
        "        corpus = corpus.reshape([-1, crop_len])\n",
        "\n",
        "        if num_batches < 10:\n",
        "            raise ValueError(\n",
        "                f\"Seulement {num_batches} lots ; envisagez une s√©quence plus courte \"\n",
        "                \"ou un lot plus petit.\"\n",
        "            )\n",
        "\n",
        "        self._ds = WordBasedAsciiDatasetForLLM._infinite_shuffle(\n",
        "            corpus, batch_size * 10\n",
        "        )\n",
        "\n",
        "    def __iter__(self):\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"G√©n√©rer le prochain mini-lot.\"\"\"\n",
        "        batch = [next(self._ds) for _ in range(self._batch_size)]\n",
        "        batch = np.stack(batch)\n",
        "        # Cr√©er les paires observation/cible pour la mod√©lisation du langage.\n",
        "        return dict(\n",
        "            input=batch[:, :-1], target=batch[:, 1:]\n",
        "        )\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        \"\"\"Convertir une s√©quence d'IDs de mots en mots.\"\"\"\n",
        "        return [self.id_to_word[id] for id in ids]\n",
        "\n",
        "    @staticmethod\n",
        "    def _infinite_shuffle(iterable, buffer_size):\n",
        "        \"\"\"R√©p√©ter et m√©langer infiniment les donn√©es de l'it√©rable.\"\"\"\n",
        "        ds = itertools.cycle(iterable)\n",
        "        buf = [next(ds) for _ in range(buffer_size)]\n",
        "        random.shuffle(buf)\n",
        "        while True:\n",
        "            item = next(ds)\n",
        "            idx = random.randint(0, buffer_size - 1)  # Inclus.\n",
        "            result, buf[idx] = buf[idx], item\n",
        "            yield result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WBIFg51oQl0"
      },
      "source": [
        "Lets now look how our data is structured for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WvH3XPM5_RGB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289d6abb-ff9b-40e6-d657-b14ebd56a00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------- Entr√©e -----------\n",
            "TEXTE : yourselves? First Citizen: We cannot, sir, we are undone already. MENENIUS: I tell you, friends, most charitable care Have the patricians of you. For your wants, Your suffering in this dearth, you\n",
            "ASCII : [ 7165 21225  6386 12051 23400 20648 13970 24075 10855 21358  6834 18610\n",
            " 18866 10949 18923  5916  6643 23574  8974 18551 11030 11271 11796 16902\n",
            " 14743 10072 18124 14937  2720 24085 20056 24497]\n",
            "---------- Cible ----------\n",
            "TEXTE : First Citizen: We cannot, sir, we are undone already. MENENIUS: I tell you, friends, most charitable care Have the patricians of you. For your wants, Your suffering in this dearth, you may\n",
            "ASCII : [21225  6386 12051 23400 20648 13970 24075 10855 21358  6834 18610 18866\n",
            " 10949 18923  5916  6643 23574  8974 18551 11030 11271 11796 16902 14743\n",
            " 10072 18124 14937  2720 24085 20056 24497  1467]\n",
            "---------- Entr√©e -----------\n",
            "TEXTE : talking on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if\n",
            "ASCII : [23040  9771  5710  6845  3195 12720 23890 25627  6832  6386 19964 22626\n",
            "  2771 14683 21225  6386 12051 24075 18486  9009 23431 18551 11030  1356\n",
            " 10522 16499  8936  3618 13487 14241  3101 11828]\n",
            "---------- Cible ----------\n",
            "TEXTE : on't; let it be done: away, away! Second Citizen: One word, good citizens. First Citizen: We are accounted poor citizens, the patricians good. What authority surfeits on would relieve us: if they\n",
            "ASCII : [ 9771  5710  6845  3195 12720 23890 25627  6832  6386 19964 22626  2771\n",
            " 14683 21225  6386 12051 24075 18486  9009 23431 18551 11030  1356 10522\n",
            " 16499  8936  3618 13487 14241  3101 11828 10895]\n",
            "\n",
            " Taille totale du vocabulaire : 25670\n"
          ]
        }
      ],
      "source": [
        "# √âchantillonner et examiner les donn√©es\n",
        "batch_size = 2\n",
        "seq_length = 32\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "\n",
        "batch = next(train_dataset)\n",
        "\n",
        "for obs, target in zip(batch[\"input\"], batch[\"target\"]):\n",
        "    print(\"-\" * 10, \"Entr√©e\", \"-\" * 11)\n",
        "    print(\"TEXTE :\", ' '.join(train_dataset.ids_to_words(obs)))\n",
        "    print(\"ASCII :\", obs)\n",
        "    print(\"-\" * 10, \"Cible\", \"-\" * 10)\n",
        "    print(\"TEXTE :\", ' '.join(train_dataset.ids_to_words(target)))\n",
        "    print(\"ASCII :\", target)\n",
        "\n",
        "print(f\"\\n Taille totale du vocabulaire : {train_dataset.vocab_size}\")\n",
        "\n",
        "VOCAB_SIZE = train_dataset.vocab_size\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9vzee53_RGB"
      },
      "source": [
        "Ensuite, entra√Ænons notre LLM et voyons comment il se comporte pour produire du texte shakespearien. Tout d'abord, nous allons d√©finir ce qui se passe √† chaque √©tape d'entra√Ænement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGuYBCkekgDw"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(3, 4))\n",
        "def train_step(params, optimizer_state, batch, apply_fn, update_fn):\n",
        "    \"\"\"\n",
        "    Effectuer une √©tape d'entra√Ænement.\n",
        "\n",
        "    Args:\n",
        "        params: Les param√®tres actuels du mod√®le.\n",
        "        optimizer_state: L'√©tat actuel de l'optimiseur.\n",
        "        batch: Un dictionnaire contenant les donn√©es d'entr√©e et les √©tiquettes cibles pour le batch.\n",
        "        apply_fn: La fonction utilis√©e pour appliquer le mod√®le aux entr√©es.\n",
        "        update_fn: La fonction utilis√©e pour mettre √† jour les param√®tres du mod√®le en fonction des gradients.\n",
        "\n",
        "    Returns:\n",
        "        Param√®tres mis √† jour, √©tat de l'optimiseur mis √† jour, et la perte calcul√©e pour le batch.\n",
        "    \"\"\"\n",
        "\n",
        "    def loss_fn(params):\n",
        "        # Obtenez la longueur de la s√©quence (T) √† partir des donn√©es d'entr√©e.\n",
        "        T = batch['input'].shape[1]\n",
        "\n",
        "        # Appliquez le mod√®le aux donn√©es d'entr√©e, en utilisant un masque triangulaire inf√©rieur pour imposer la causalit√©.\n",
        "        # jnp.tril(np.ones((T, T))) cr√©e une matrice triangulaire inf√©rieure de uns.\n",
        "        logits = apply_fn(params, batch['input'], jnp.tril(np.ones((T, T))))\n",
        "\n",
        "        # Calculez la perte entre les logits pr√©dits et les √©tiquettes cibles.\n",
        "        loss = sequence_loss_fn(logits, batch['target'])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Calculez la perte et ses gradients par rapport aux param√®tres.\n",
        "    loss, gradients = jax.value_and_grad(loss_fn)(params)\n",
        "\n",
        "    # Mettez √† jour l'√©tat de l'optimiseur et calculez les mises √† jour des param√®tres en fonction des gradients.\n",
        "    updates, optimizer_state = update_fn(gradients, optimizer_state)\n",
        "\n",
        "    # Appliquez les mises √† jour aux param√®tres.\n",
        "    params = optax.apply_updates(params, updates)\n",
        "\n",
        "    # Retournez les param√®tres mis √† jour, l'√©tat de l'optimiseur, et la perte pour le batch.\n",
        "    return params, optimizer_state, loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtKWzKIAkfYU"
      },
      "source": [
        "Nous allons maintenant initialiser notre optimiseur et notre mod√®le. N'h√©sitez pas √† exp√©rimenter avec les hyperparam√®tres pendant la pratique.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def return_frequency_pe_matrix(longueur_sequence_tokens, taille_embedding_tokens):\n",
        "\n",
        "    assert taille_embedding_tokens % 2 == 0, \"la taille de l'embedding des tokens doit √™tre divisible par deux\"\n",
        "\n",
        "    P = jnp.zeros((longueur_sequence_tokens, taille_embedding_tokens))\n",
        "    positions = jnp.arange(0, longueur_sequence_tokens)[:, jnp.newaxis]\n",
        "\n",
        "    i = jnp.arange(0, taille_embedding_tokens, 2)\n",
        "    pas_frequence = jnp.exp(i * (-math.log(10000.0) / taille_embedding_tokens))\n",
        "    frequences = positions * pas_frequence\n",
        "\n",
        "    P = P.at[:, 0::2].set(jnp.sin(frequences))\n",
        "    P = P.at[:, 1::2].set(jnp.cos(frequences))\n",
        "\n",
        "    return P"
      ],
      "metadata": {
        "id": "VgwaEmdHGJ5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None):\n",
        "    \"\"\"\n",
        "    Attention √† produit scalaire √©chelonn√© avec un masque causal\n",
        "    (se concentrant uniquement sur les positions pr√©c√©dentes)\n",
        "    \"\"\"\n",
        "    d_k = key.shape[-1]\n",
        "    T_k = key.shape[-2]\n",
        "    T_q = query.shape[-2]\n",
        "\n",
        "    # obtenir les logits √©chelonn√©s en utilisant le produit scalaire comme pr√©c√©demment\n",
        "    logits = jnp.matmul(query, jnp.swapaxes(key, -2, -1))\n",
        "    scaled_logits = logits / jnp.sqrt(d_k)\n",
        "\n",
        "    # ajouter un masque optionnel o√π les valeurs le long du masque sont d√©finies √† -inf\n",
        "    if mask is not None:\n",
        "        scaled_logits = jnp.where(mask[:T_q, :T_k], scaled_logits, -jnp.inf)\n",
        "\n",
        "    # calculer les poids d'attention via softmax\n",
        "    attention_weights = jax.nn.softmax(scaled_logits, axis=-1)\n",
        "\n",
        "    # faire la somme avec les valeurs pour obtenir la sortie\n",
        "    output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "metadata": {
        "id": "1wsXLjJ9FWrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SequenceToQKV(nn.Module):\n",
        "  taille_sortie: int\n",
        "\n",
        "  @nn.compact\n",
        "  def __call__(self, X):\n",
        "\n",
        "    # d√©finir la m√©thode d'initialisation des poids\n",
        "    initialisateur = nn.initializers.variance_scaling(scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\")\n",
        "\n",
        "    # initialiser trois couches lin√©aires pour faire les transformations QKV.\n",
        "    # note : cela pourrait aussi √™tre une seule couche, comment pensez-vous que vous le feriez ?\n",
        "    couche_q = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "    couche_k = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "    couche_v = nn.Dense(self.taille_sortie, kernel_init=initialisateur)\n",
        "\n",
        "    # transformer et retourner les matrices\n",
        "    Q = couche_q(X)\n",
        "    K = couche_k(X)\n",
        "    V = couche_v(X)\n",
        "\n",
        "    return Q, K, V\n"
      ],
      "metadata": {
        "id": "WuHCXSYKFp4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    num_heads: int  # Nombre de t√™tes d'attention\n",
        "    d_m: int  # Dimension des embeddings du mod√®le\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialiser le module de transformation de la s√©quence en QKV (requ√™te, cl√©, valeur)\n",
        "        self.sequence_to_qkv = SequenceToQKV(self.d_m)\n",
        "\n",
        "        # D√©finir l'initialiseur pour les poids de la couche lin√©aire de sortie\n",
        "        initializer = nn.initializers.variance_scaling(\n",
        "            scale=0.5, mode=\"fan_in\", distribution=\"truncated_normal\"\n",
        "        )\n",
        "\n",
        "        # Initialiser la couche de projection de sortie Wo (utilis√©e apr√®s l'attention)\n",
        "        self.Wo = nn.Dense(self.d_m, kernel_init=initializer)\n",
        "\n",
        "    def __call__(self, X=None, Q=None, K=None, V=None, mask=None, return_weights=False):\n",
        "        # Si Q, K ou V ne sont pas fournis, utiliser l'entr√©e X pour les g√©n√©rer\n",
        "        if None in [Q, K, V]:\n",
        "            assert not X is None, \"X doit √™tre fourni si Q, K ou V ne sont pas fournis\"\n",
        "\n",
        "            # G√©n√©rer les matrices Q, K et V √† partir de l'entr√©e X\n",
        "            Q, K, V = self.sequence_to_qkv(X)\n",
        "\n",
        "        # Extraire la taille du lot (B), la longueur de la s√©quence (T) et la taille de l'embedding (d_m)\n",
        "        B, T, d_m = K.shape\n",
        "\n",
        "        # Calculer la taille de l'embedding de chaque t√™te d'attention (d_m / num_heads)\n",
        "        head_size = d_m // self.num_heads\n",
        "\n",
        "        # Reshaper Q, K, V pour avoir des dimensions s√©par√©es pour les t√™tes\n",
        "        # B, T, d_m -> B, T, num_heads, head_size -> B, num_heads, T, head_size\n",
        "        q_heads = Q.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        k_heads = K.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "        v_heads = V.reshape(B, T, self.num_heads, head_size).swapaxes(1, 2)\n",
        "\n",
        "        # Appliquer l'attention √† produit scalaire √©chelonn√© √† chaque t√™te\n",
        "        attention, attention_weights = scaled_dot_product_attention(\n",
        "            q_heads, k_heads, v_heads, mask\n",
        "        )\n",
        "\n",
        "        # Reshaper la sortie de l'attention √† ses dimensions originales\n",
        "        # (B, num_heads, T, head_size) -> (B, T, num_heads, head_size) -> (B, T, d_m)\n",
        "        attention = attention.swapaxes(1, 2).reshape(B, T, d_m)\n",
        "\n",
        "        # Appliquer la transformation lin√©aire de sortie Wo √† la sortie de l'attention\n",
        "        X_new = self.Wo(attention)\n",
        "\n",
        "        # Si return_weights est True, retourner √† la fois la sortie transform√©e et les poids d'attention\n",
        "        if return_weights:\n",
        "            return X_new, attention_weights\n",
        "        else:\n",
        "            # Sinon, retourner uniquement la sortie transform√©e\n",
        "            return X_new\n"
      ],
      "metadata": {
        "id": "gJyjEbeMFVUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddNorm(nn.Module):\n",
        "    \"\"\"Un bloc qui impl√©mente l'op√©ration 'Add and Norm' utilis√©e dans les transformers.\"\"\"\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x, processed_x):\n",
        "      # √âtape 1 : Ajouter l'entr√©e originale (x) √† l'entr√©e trait√©e (processed_x).\n",
        "      added = x + processed_x\n",
        "\n",
        "      # √âtape 2 : Appliquer une normalisation par couche au r√©sultat de l'addition.\n",
        "      # - LayerNorm aide √† stabiliser et am√©liorer le processus d'entra√Ænement en normalisant la sortie.\n",
        "      # - reduction_axes=-1 indique que la normalisation est appliqu√©e sur la derni√®re dimension (g√©n√©ralement la dimension de l'embedding).\n",
        "      # - use_scale=True et use_bias=True permettent √† la couche d'apprendre des param√®tres d'√©chelle et de biais pour un ajustement plus pr√©cis.\n",
        "      normalised = nn.LayerNorm(reduction_axes=-1, use_scale=True, use_bias=True)\n",
        "\n",
        "      # Retourner le r√©sultat normalis√©.\n",
        "      return normalised(added)"
      ],
      "metadata": {
        "id": "IREB57lyFIuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardBlock(nn.Module):\n",
        "    \"\"\"Un MLP (Multi-Layer Perceptron) √† 2 couches qui commence par √©largir la taille de l'entr√©e, puis la r√©duit √† nouveau.\"\"\"\n",
        "\n",
        "    # widening_factor contr√¥le l'expansion de la dimension de l'entr√©e dans la premi√®re couche.\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    # init_scale contr√¥le le facteur d'√©chelle pour l'initialisation des poids.\n",
        "    init_scale: float = 0.25\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, x):\n",
        "      # Obtenir la taille de la derni√®re dimension de l'entr√©e (taille de l'embedding).\n",
        "      d_m = x.shape[-1]\n",
        "\n",
        "      # Calculer la taille de la premi√®re couche en multipliant la taille de l'embedding par le facteur d'√©largissement.\n",
        "      layer1_size = self.widening_factor * d_m\n",
        "\n",
        "      # Initialiser les poids des deux couches en utilisant un initialiseur bas√© sur la variance.\n",
        "      initializer = nn.initializers.variance_scaling(\n",
        "          scale=self.init_scale, mode='fan_in', distribution='truncated_normal',\n",
        "      )\n",
        "\n",
        "      # D√©finir la premi√®re couche dense, qui √©largit la taille de l'entr√©e.\n",
        "      layer1 = nn.Dense(layer1_size, kernel_init=initializer)\n",
        "\n",
        "      # D√©finir la deuxi√®me couche dense, qui r√©duit la taille pour revenir √† la dimension d'origine.\n",
        "      layer2 = nn.Dense(d_m, kernel_init=initializer)\n",
        "\n",
        "      # Appliquer la premi√®re couche dense suivie d'une fonction d'activation GELU.\n",
        "      x = jax.nn.gelu(layer1(x))\n",
        "\n",
        "      # Appliquer la deuxi√®me couche dense pour ramener les donn√©es √† leur dimension d'origine.\n",
        "      x = layer2(x)\n",
        "\n",
        "      # Retourner la sortie finale.\n",
        "      return x\n"
      ],
      "metadata": {
        "id": "ADg7Y3ZqFUKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bloc d√©codeur du Transformer.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Le nombre de t√™tes d'attention dans le bloc Multi-Head\n",
        "        Attention (MHA).\n",
        "        d_m: La taille des embeddings des tokens.\n",
        "        widening_factor: Le facteur par lequel la taille de la couche cach√©e\n",
        "        est augment√©e dans le MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    num_heads: int\n",
        "    d_m: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "      # Initialiser le bloc Multi-Head Attention (MHA)\n",
        "      self.mha = MultiHeadAttention(self.num_heads, self.d_m)\n",
        "\n",
        "      # Initialiser les blocs AddNorm pour les connexions r√©siduelles\n",
        "      # et la normalisation\n",
        "      self.add_norm1 = AddNorm()  # Premier bloc AddNorm apr√®s MHA\n",
        "      self.add_norm2 = AddNorm()  # Deuxi√®me bloc AddNorm apr√®s le MLP\n",
        "\n",
        "      # Initialiser le FeedForwardBlock (MLP) qui traite les donn√©es\n",
        "      # apr√®s l'attention\n",
        "      self.MLP = FeedForwardBlock(widening_factor=self.widening_factor)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weight=True):\n",
        "      \"\"\"\n",
        "      Passage en avant √† travers le DecoderBlock.\n",
        "\n",
        "      Args:\n",
        "          X: Lot de tokens d'entr√©e envoy√©s dans le d√©codeur,\n",
        "          forme [B, T_decoder, d_m]\n",
        "          mask [optionnel, par d√©faut=None]: Masque pour contr√¥ler les positions\n",
        "          que l'attention est autoris√©e √† consid√©rer,\n",
        "          forme [T_decoder, T_decoder].\n",
        "          return_att_weight [optionnel, par d√©faut=True]: Si True,\n",
        "          retourne les poids d'attention avec la sortie.\n",
        "\n",
        "      Returns:\n",
        "          Si return_att_weight est True, retourne un tuple (X,\n",
        "          attention_weights_1).\n",
        "          Sinon, retourne les repr√©sentations des tokens trait√©s X.\n",
        "      \"\"\"\n",
        "      # Appliquer l'attention multi-t√™te aux tokens d'entr√©e (X)\n",
        "      # avec un masquage optionnel\n",
        "      attention, attention_weights_1 = self.mha(X, mask=mask, return_weights=True)\n",
        "\n",
        "      # Appliquer le premier bloc AddNorm (ajoute l'entr√©e originale X\n",
        "      # et normalise)\n",
        "      X = self.add_norm1(X, attention)\n",
        "\n",
        "      # Passer le r√©sultat √† travers le FeedForwardBlock (MLP)\n",
        "      # pour traiter davantage les donn√©es\n",
        "      projection = self.MLP(X)\n",
        "\n",
        "      # Appliquer le deuxi√®me bloc AddNorm (ajoute l'entr√©e de l'√©tape\n",
        "      # pr√©c√©dente et normalise)\n",
        "      X = self.add_norm2(X, projection)\n",
        "\n",
        "      # Retourner la sortie finale X, et √©ventuellement les poids d'attention\n",
        "      return (X, attention_weights_1) if return_att_weight else X"
      ],
      "metadata": {
        "id": "Po2EMcSTEnQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLM(nn.Module):\n",
        "    \"\"\"\n",
        "    Mod√®le Transformer compos√© de plusieurs couches de blocs d√©codeurs.\n",
        "\n",
        "    Args:\n",
        "        num_heads: Nombre de t√™tes d'attention dans chaque bloc Multi-Head\n",
        "        Attention (MHA).\n",
        "        num_layers: Nombre de blocs d√©codeurs dans le mod√®le.\n",
        "        d_m: Dimensionnalit√© des embeddings des tokens.\n",
        "        vocab_size: Taille du vocabulaire (nombre de tokens uniques).\n",
        "        widening_factor: Facteur par lequel la taille de la couche cach√©e\n",
        "        est augment√©e dans le MLP.\n",
        "    \"\"\"\n",
        "    num_heads: int\n",
        "    num_layers: int\n",
        "    d_m: int\n",
        "    vocab_size: int\n",
        "    widening_factor: int = 4\n",
        "\n",
        "    def setup(self):\n",
        "        # Initialiser une liste de blocs d√©codeurs, un pour chaque\n",
        "        # couche du mod√®le\n",
        "        self.blocks = [\n",
        "            DecoderBlock(self.num_heads, self.d_m, self.widening_factor)\n",
        "            for _ in range(self.num_layers)\n",
        "        ]\n",
        "\n",
        "        # Initialiser une couche d'embedding pour convertir les IDs de\n",
        "        # tokens en embeddings de tokens\n",
        "        self.embedding = nn.Embed(num_embeddings=self.vocab_size, features=self.d_m)\n",
        "\n",
        "        # Initialiser une couche dense pour pr√©dire le prochain token\n",
        "        # dans la s√©quence\n",
        "        self.pred_layer = nn.Dense(self.vocab_size)\n",
        "\n",
        "    def __call__(self, X, mask=None, return_att_weights=False):\n",
        "        \"\"\"\n",
        "        Passage en avant √† travers le mod√®le LLM.\n",
        "\n",
        "        Args:\n",
        "            X: Lot d'IDs de tokens d'entr√©e, forme [B, T_decoder]\n",
        "            o√π B est la taille du lot et T_decoder est la longueur\n",
        "            de la s√©quence.\n",
        "            mask [optionnel, par d√©faut=None]: Masque pour contr√¥ler les\n",
        "            positions sur lesquelles l'attention peut se concentrer,\n",
        "            forme [T_decoder, T_decoder].\n",
        "            return_att_weights [optionnel, par d√©faut=False]: Indique\n",
        "            si les poids d'attention doivent √™tre retourn√©s.\n",
        "\n",
        "        Returns:\n",
        "            logits: Les probabilit√©s pr√©dites pour chaque token dans\n",
        "            le vocabulaire.\n",
        "            Si return_att_weights est True, retourne √©galement\n",
        "            les poids d'attention.\n",
        "        \"\"\"\n",
        "\n",
        "        # Convertir les IDs de tokens en embeddings (forme\n",
        "        # [B, T_decoder, d_m])\n",
        "        X = self.embedding(X)\n",
        "\n",
        "        # Obtenir la longueur de la s√©quence d'entr√©e\n",
        "        sequence_len = X.shape[-2]\n",
        "\n",
        "        # G√©n√©rer des encodages positionnels et les ajouter aux\n",
        "        # embeddings des tokens\n",
        "        positions = return_frequency_pe_matrix(sequence_len, self.d_m)\n",
        "        X = X + positions\n",
        "\n",
        "        # Initialiser une liste pour stocker les poids d'attention\n",
        "        # si n√©cessaire\n",
        "        if return_att_weights:\n",
        "            att_weights = []\n",
        "\n",
        "        # Passer les embeddings √† travers chaque bloc d√©codeur\n",
        "        # en s√©quence\n",
        "        for block in self.blocks:\n",
        "            out = block(X, mask, return_att_weights)\n",
        "            if return_att_weights:\n",
        "                # Si on retourne les poids d'attention, d√©baller la sortie\n",
        "                X = out[0]\n",
        "                att_weights.append(out[1])\n",
        "            else:\n",
        "                # Sinon, mettre √† jour simplement l'entr√©e pour le\n",
        "                # bloc suivant\n",
        "                X = out\n",
        "\n",
        "        # Appliquer une couche dense suivie d'un log softmax pour obtenir\n",
        "        # les logits (probabilit√©s pr√©dites des tokens)\n",
        "        logits = nn.log_softmax(self.pred_layer(X))\n",
        "\n",
        "        # Retourner les logits, et √©ventuellement, les poids d'attention\n",
        "        return logits if not return_att_weights else (logits, jnp.array(att_weights).swapaxes(0, 1))"
      ],
      "metadata": {
        "id": "ieW0qU66DhHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8o3q-BZX_RGB"
      },
      "outputs": [],
      "source": [
        "# D√©finir tous les hyperparam√®tres\n",
        "d_model = 128            # Dimension des embeddings de tokens (d_m)\n",
        "num_heads = 4            # Nombre de t√™tes d'attention dans Multi-Head Attention\n",
        "num_layers = 1           # Nombre de blocs d√©codeurs dans le mod√®le\n",
        "widening_factor = 2      # Facteur d'√©largissement de la taille de la couche cach√©e dans le MLP\n",
        "LR = 2e-3                # Taux d'apprentissage pour l'optimiseur\n",
        "batch_size = 32          # Nombre d'√©chantillons par lot d'entra√Ænement\n",
        "seq_length = 64          # Longueur de chaque s√©quence d'entr√©e (nombre de tokens)\n",
        "\n",
        "# Pr√©parer les donn√©es d'entra√Ænement\n",
        "train_dataset = WordBasedAsciiDatasetForLLM(\"input.txt\", batch_size, seq_length)\n",
        "vocab_size = train_dataset.vocab_size  # Obtenir la taille du vocabulaire √† partir du dataset\n",
        "batch = next(train_dataset)            # Obtenir le premier lot de donn√©es d'entr√©e\n",
        "\n",
        "# D√©finir la cl√© du g√©n√©rateur de nombres al√©atoires pour l'initialisation du mod√®le\n",
        "rng = jax.random.PRNGKey(42)\n",
        "\n",
        "# Initialiser le mod√®le LLM avec les hyperparam√®tres sp√©cifi√©s\n",
        "llm = LLM(num_heads=num_heads, num_layers=num_layers, d_m=d_model, vocab_size=vocab_size, widening_factor=widening_factor)\n",
        "\n",
        "# Cr√©er un masque causal pour s'assurer que le mod√®le ne se concentre que sur les tokens pr√©c√©dents\n",
        "mask = jnp.tril(np.ones((batch['input'].shape[1], batch['input'].shape[1])))\n",
        "\n",
        "# Initialiser les param√®tres du mod√®le en utilisant le premier lot de donn√©es d'entr√©e et le masque\n",
        "params = llm.init(rng, batch['input'], mask)\n",
        "\n",
        "# Configurer l'optimiseur en utilisant l'algorithme d'optimisation Adam avec le taux d'apprentissage sp√©cifi√©\n",
        "optimizer = optax.adam(LR, b1=0.9, b2=0.99)\n",
        "optimizer_state = optimizer.init(params)  # Initialiser l'√©tat de l'optimiseur avec les param√®tres du mod√®le"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bPEFakxmvsM"
      },
      "source": [
        "Now we train! This will take a few minutes..\n",
        "While it trains, have you greeted your neighbor yet?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUAS6tie_RGB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "outputId": "f4f87d1a-5857-4a92-f7d9-6152507e58d6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x800 with 2 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"440.08pt\" height=\"567.034375pt\" viewBox=\"0 0 440.08 567.034375\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2024-11-23T12:51:46.498539</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.8.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 567.034375 \nL 440.08 567.034375 \nL 440.08 0 \nL 0 0 \nz\n\" style=\"fill: #ffffff\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 529.478125 \nL 432.88 529.478125 \nL 432.88 22.318125 \nL 26.925 22.318125 \nz\n\" style=\"fill: #ffffff\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"mc380e9f41f\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"45.3775\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(42.19625 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"113.093096\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20 -->\n      <g transform=\"translate(106.730596 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"180.808693\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40 -->\n      <g transform=\"translate(174.446193 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"248.524289\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60 -->\n      <g transform=\"translate(242.161789 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-36\" d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"316.239885\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80 -->\n      <g transform=\"translate(309.877385 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-38\" d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#mc380e9f41f\" x=\"383.955482\" y=\"529.478125\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100 -->\n      <g transform=\"translate(374.411732 544.076562) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- epoch -->\n     <g transform=\"translate(214.674375 557.754687) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-65\"/>\n      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"m200438ecda\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"513.546079\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 517.345298) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"425.803674\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 2 -->\n      <g transform=\"translate(13.5625 429.602893) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"338.06127\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 4 -->\n      <g transform=\"translate(13.5625 341.860488) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"250.318865\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 6 -->\n      <g transform=\"translate(13.5625 254.118084) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-36\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"162.57646\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 8 -->\n      <g transform=\"translate(13.5625 166.375679) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-38\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#m200438ecda\" x=\"26.925\" y=\"74.834056\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 78.633274) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_13\">\n    <path d=\"M 45.3775 45.370852 \nL 48.76328 144.1198 \nL 52.14906 166.375992 \nL 55.534839 168.084376 \nL 58.920619 178.222889 \nL 62.306399 177.676557 \nL 65.692179 181.470838 \nL 69.077959 187.108308 \nL 72.463739 190.22572 \nL 75.849518 199.233559 \nL 79.235298 215.268127 \nL 82.621078 223.30884 \nL 86.006858 236.038098 \nL 89.392638 253.972483 \nL 92.778417 261.093475 \nL 96.164197 270.665971 \nL 99.549977 286.284033 \nL 102.935757 293.037365 \nL 106.321537 298.534529 \nL 109.707317 312.203445 \nL 113.093096 317.308578 \nL 116.478876 320.493204 \nL 119.864656 331.959996 \nL 123.250436 336.491037 \nL 126.636216 338.328337 \nL 130.021995 347.390787 \nL 133.407775 351.778436 \nL 136.793555 353.349516 \nL 140.179335 361.789038 \nL 143.565115 366.366709 \nL 146.950894 367.876756 \nL 150.336674 374.93023 \nL 153.722454 380.849871 \nL 157.108234 382.782543 \nL 160.494014 389.892165 \nL 163.879794 394.615498 \nL 167.265573 396.559926 \nL 170.651353 403.760066 \nL 174.037133 409.559107 \nL 177.422913 410.540029 \nL 180.808693 416.375313 \nL 184.194472 422.582 \nL 187.580252 423.198746 \nL 190.966032 429.559876 \nL 194.351812 433.369099 \nL 197.737592 433.810383 \nL 201.123372 439.870875 \nL 204.509151 444.14794 \nL 207.894931 443.82714 \nL 211.280711 449.43106 \nL 214.666491 453.525367 \nL 218.052271 453.51359 \nL 221.43805 457.854992 \nL 224.82383 461.175416 \nL 228.20961 461.793386 \nL 231.59539 465.393027 \nL 234.98117 468.528451 \nL 238.36695 468.35529 \nL 241.752729 472.943567 \nL 245.138509 474.560095 \nL 248.524289 474.415798 \nL 251.910069 477.981126 \nL 255.295849 480.744827 \nL 258.681628 480.637442 \nL 262.067408 482.866657 \nL 265.453188 485.277273 \nL 268.838968 484.880152 \nL 272.224748 487.115465 \nL 275.610528 489.416528 \nL 278.996307 489.170202 \nL 282.382087 490.363602 \nL 285.767867 492.328863 \nL 289.153647 492.963073 \nL 292.539427 494.016575 \nL 295.925206 494.975295 \nL 299.310986 495.561907 \nL 302.696766 496.395612 \nL 306.082546 497.042082 \nL 309.468326 498.070966 \nL 312.854106 498.404025 \nL 316.239885 499.283441 \nL 319.625665 499.66618 \nL 323.011445 499.857643 \nL 326.397225 500.756657 \nL 329.783005 501.236457 \nL 333.168784 501.102416 \nL 336.554564 501.900372 \nL 339.940344 503.035016 \nL 343.326124 502.82153 \nL 346.711904 503.148343 \nL 350.097683 503.685165 \nL 353.483463 503.531434 \nL 356.869243 503.725872 \nL 360.255023 503.841052 \nL 363.640803 503.736974 \nL 367.026583 504.241687 \nL 370.412362 504.717279 \nL 373.798142 504.744642 \nL 377.183922 505.081933 \nL 380.569702 505.812153 \nL 383.955482 505.192313 \nL 387.341261 505.563609 \nL 390.727041 505.876506 \nL 394.112821 505.869548 \nL 397.498601 506.061056 \nL 400.884381 506.265981 \nL 404.270161 505.679928 \nL 407.65594 505.879926 \nL 411.04172 506.425398 \nL 414.4275 506.269135 \n\" clip-path=\"url(#p6eacdd66ad)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 529.478125 \nL 26.925 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 432.88 529.478125 \nL 432.88 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 529.478125 \nL 432.88 529.478125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 22.318125 \nL 432.88 22.318125 \n\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"text_14\">\n    <!-- Loss -->\n    <g transform=\"translate(216.741875 16.318125) scale(0.12 -0.12)\">\n     <defs>\n      <path id=\"DejaVuSans-4c\" d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n      <path id=\"DejaVuSans-73\" d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" transform=\"scale(0.015625)\"/>\n     </defs>\n     <use xlink:href=\"#DejaVuSans-4c\"/>\n     <use xlink:href=\"#DejaVuSans-6f\" x=\"53.962891\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"115.144531\"/>\n     <use xlink:href=\"#DejaVuSans-73\" x=\"167.244141\"/>\n    </g>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 374.564375 284.737187 \nL 425.88 284.737187 \nQ 427.88 284.737187 427.88 282.737187 \nL 427.88 269.059062 \nQ 427.88 267.059062 425.88 267.059062 \nL 374.564375 267.059062 \nQ 372.564375 267.059062 372.564375 269.059062 \nL 372.564375 282.737187 \nQ 372.564375 284.737187 374.564375 284.737187 \nz\n\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n    </g>\n    <g id=\"line2d_14\">\n     <path d=\"M 376.564375 275.1575 \nL 386.564375 275.1575 \nL 396.564375 275.1575 \n\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- loss -->\n     <g transform=\"translate(404.564375 278.6575) scale(0.1 -0.1)\">\n      <defs>\n       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use xlink:href=\"#DejaVuSans-6f\" x=\"27.783203\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"88.964844\"/>\n      <use xlink:href=\"#DejaVuSans-73\" x=\"141.064453\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6eacdd66ad\">\n   <rect x=\"26.925\" y=\"22.318125\" width=\"405.955\" height=\"507.16\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss\n",
            "\tloss             \t (min:    0.162, max:   10.672, cur:    0.166)\n"
          ]
        }
      ],
      "source": [
        "plotlosses = PlotLosses()\n",
        "\n",
        "MAX_STEPS = 3500\n",
        "LOG_EVERY = 32\n",
        "losses = []\n",
        "VOCAB_SIZE = 25670\n",
        "\n",
        "# Boucle d'entra√Ænement\n",
        "for step in range(MAX_STEPS):\n",
        "    batch = next(train_dataset)\n",
        "    params, optimizer_state, loss = train_step(\n",
        "        params, optimizer_state, batch, llm.apply, optimizer.update)\n",
        "    losses.append(loss)\n",
        "    if step % LOG_EVERY == 0:\n",
        "        loss_ = jnp.array(losses).mean()\n",
        "        plotlosses.update(\n",
        "            {\n",
        "                \"loss\": loss_,\n",
        "            }\n",
        "        )\n",
        "        plotlosses.send()\n",
        "        losses = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGv9c2AFmF4V"
      },
      "source": [
        "### 3. Inspecter le LLM entra√Æn√© <font color='orange'>D√©butant</font>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pfq61gim_RGB"
      },
      "source": [
        "**Rappel :** n'oubliez pas d'ex√©cuter tout le code pr√©sent√© jusqu'√† pr√©sent dans cette section avant de lancer les cellules ci-dessous !\n",
        "\n",
        "G√©n√©rons maintenant un peu de texte et voyons comment notre mod√®le a perform√©. NE STOPPEZ PAS LA CELLULE UNE FOIS QU'ELLE EST EN COURS D'EX√âCUTION, CELA FERA PLANTER LA SESSION.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lt8HTS__RGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c0a317-a3cf-4c23-bdb1-b8554b33dc25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "life an hour Hath been a lamb indeed, that baes like a bear. MENENIUS: He's a"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "\n",
        "@functools.partial(jax.jit, static_argnums=(2, ))\n",
        "def generate_prediction(params, input, apply_fn):\n",
        "  logits = apply_fn(params, input)\n",
        "  argmax_out = jnp.argmax(logits, axis=-1)\n",
        "  return argmax_out[0][-1].astype(int)\n",
        "\n",
        "def generate_random_shakespeare(llm, params, id_2_word, word_2_id):\n",
        "    '''\n",
        "    Get the model output\n",
        "    '''\n",
        "\n",
        "    prompt = \"life\"\n",
        "    print(prompt, end=\"\")\n",
        "    tokens = prompt.split()\n",
        "\n",
        "    # predict and append\n",
        "    for i in range(15):\n",
        "      input = jnp.array([[word_2_id[t] for t in tokens]]).astype(int)\n",
        "      prediction = generate_prediction(params, input, llm.apply)\n",
        "      prediction = id_2_word[int(prediction)]\n",
        "      tokens.append(prediction)\n",
        "      print(\" \"+prediction, end=\"\")\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "id_2_word = train_dataset.id_to_word\n",
        "word_2_id = train_dataset.word_to_id\n",
        "\n",
        "generated_shakespeare = generate_random_shakespeare(llm, params, id_2_word, word_2_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOwNuMRf_RGC"
      },
      "source": [
        "Enfin, nous avons impl√©ment√© tout ce qui pr√©c√®de en prenant l'ID de jeton avec la probabilit√© maximale d'√™tre correct. C'est ce qu'on appelle le d√©codage gourmand, car nous avons uniquement pris le jeton le plus probable. Cela a bien fonctionn√© dans ce cas, mais il y a des situations o√π cette approche gourmande peut d√©grader les performances, notamment lorsque nous souhaitons g√©n√©rer un texte r√©aliste.\n",
        "\n",
        "Il existe d'autres m√©thodes pour √©chantillonner √† partir du d√©codeur, avec un algorithme c√©l√®bre √©tant la recherche par faisceau (beam search). Nous fournissons ci-dessous des ressources pour ceux qui souhaitent en savoir plus √† ce sujet.\n",
        "\n",
        "[D√©codage Gourmand](https://www.youtube.com/watch?v=DW5C3eqAFQM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=4)\n",
        "\n",
        "[Recherche par Faisceau](https://www.youtube.com/watch?v=uG3xoYNo3HM&list=PLmZlBIcArwhPHmHzyM_cZJQ8_v5paQJTV&index=5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**"
      ],
      "metadata": {
        "id": "15296-QL3-y3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV3YG7QOZD-B"
      },
      "source": [
        "**R√©sum√© :**\n",
        "\n",
        "Vous avez maintenant ma√Ætris√© l'essentiel du fonctionnement d'un Large Language Model (LLM) ! Ces outils puissants ont le potentiel de transformer un large √©ventail de t√¢ches. Cependant, comme tout mod√®le de deep learning, leur efficacit√© r√©side dans leur application aux bons probl√®mes avec les bonnes donn√©es.\n",
        "\n",
        "Pr√™t √† passer au niveau sup√©rieur ? Plongez dans le fine-tuning de vos propres LLMs et lib√©rez encore plus de potentiel ! Je vous recommande vivement d'explorer le tutoriel de l'ann√©e derni√®re sur les m√©thodes de fine-tuning efficaces pour obtenir une vue d'ensemble des techniques avanc√©es. Le voyage ne s'arr√™te pas l√†‚Äîil y a encore tant √† d√©couvrir ! [LLMs pour Tous 2023](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "Le monde des LLMs est √† vous‚Äîallez cr√©er quelque chose d'incroyable ! üåüüöÄ\n",
        "\n",
        "**Prochaines √©tapes :**\n",
        "\n",
        "[**Fine-tuning Efficace des LLMs avec Hugging Face**](https://colab.research.google.com/github/deep-learning-indaba/indaba-pracs-2023/blob/main/practicals/large_language_models.ipynb)\n",
        "\n",
        "**R√©f√©rences :** pour des r√©f√©rences suppl√©mentaires, consultez les liens mentionn√©s dans les sections sp√©cifiques de ce colab.\n",
        "\n",
        "* [Article \"Attention is all you need\"](https://arxiv.org/abs/1706.03762)\n",
        "* [Vid√©os suppl√©mentaires sur les transformers](https://www.youtube.com/playlist?list=PLmZlBIcArwhOPR2s-FIR7WoqNaBML233s)\n",
        "* [Article LoRA](https://arxiv.org/abs/2106.09685)\n",
        "* [RLHF](https://huggingface.co/blog/rlhf) (comment ChatGPT a √©t√© entra√Æn√©)\n",
        "* [Extension de la longueur du contexte](https://kaiokendev.github.io/context)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "145833166d986a8417df3c7acb65d917d84b716b5a452e57fcacdc66f1a168c9"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}